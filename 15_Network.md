# Network

This lecture lays out the **basic networking model in Kubernetes** and why a real cluster needs a dedicated networking solution (a CNI-style plugin) to meet Kubernetes’ communication requirements.

It begins with a **single-node Kubernetes cluster**. The node itself has an IP address—for example **192.168.1.2**—and that’s the address you use to reach the node (SSH into it, manage it, etc.). The instructor adds an important practical note for **minikube** users: in minikube, Kubernetes runs inside a VM created by your hypervisor, so the node IP being discussed is the IP of the **minikube VM**, not necessarily your laptop. Your laptop might have a different IP (the instructor gives an example like **192.168.1.1**), so you need to understand how your virtual machines are networked in your environment.

Inside this single node, you create a **Pod** (which contains your container). In Kubernetes, unlike “plain Docker” where an IP is typically associated with the container, the IP is associated with the **Pod**. **Each Pod gets its own internal IP address.** In the example, Pod IPs come from a **10.244.x.x** range, and the Pod is assigned something like **10.244.0.2**. The reason a Pod can get an address like this is that when Kubernetes is configured, an **internal private Pod network** is created (example given: **10.244.0.0**), and Pods are attached to this network. When you deploy multiple Pods, each gets a different IP from that internal network. Pods can talk to each other using these internal IPs, but the instructor warns that **relying on Pod IPs directly is not a good long-term approach**, because Pod IPs can change when Pods are recreated. They mention that better mechanisms for stable communication will be covered later, but for now the goal is to understand the underlying internal networking concept.

Networking is straightforward on a single node, but the lecture then asks: what happens in a **multi-node** cluster? The instructor describes two Kubernetes nodes with node IPs like **192.168.1.2** and **192.168.1.3**, and each node has a Pod. If each node independently creates its internal Pod network as **10.244.0.0**, then both nodes may assign the *same* Pod IPs (for example, each node might have a Pod with **10.244.0.2**). Once these nodes are part of the same cluster, that leads to **IP conflicts**, because Pods across the cluster must have unique addresses.

At this point, the key takeaway is that **Kubernetes does not automatically solve this networking problem by itself**. When you set up a cluster, Kubernetes expects *you* (or your platform/distribution) to provide a networking solution that satisfies a few fundamental requirements. The lecture lists these explicitly:

1. **All Pods/containers must be able to communicate with each other** across the cluster **without configuring NAT**.
2. **All nodes must be able to communicate with all Pods/containers**.
3. **All Pods/containers must be able to communicate with the nodes** in the cluster.

To meet these requirements, you typically install a **cluster networking solution** (a pre-built networking plugin). The instructor notes that you don’t have to build it yourself because many solutions exist. They list examples such as **Cisco ACI**, **Cilium**, **Big Cloud Fabric**, **Flannel**, **VMware NSX-T** (but in Kubernetes networking this is commonly **NSX-T**), and **Calico**. The choice depends on where you’re running Kubernetes: if you build a cluster from scratch on your own machines, you might choose something like **Calico** or **Flannel**; in a VMware environment, **NSX-T** can be a good fit. They also mention that in **Play with Kubernetes (Play with K8s)** labs, the networking solution used is **Weave Net**.

Finally, the lecture returns to the earlier multi-node example and explains what these solutions do in practice. With something like **Flannel** or **Calico** installed, the networking layer manages Pod networks and IP allocation across nodes by ensuring each node gets a **different Pod network range**, preventing overlaps and conflicts. This creates a single, cluster-wide **virtual network** spanning all nodes and Pods, where every Pod has a unique IP. Using routing techniques, the networking solution enables Pods on different nodes to communicate with each other, and it satisfies the core Kubernetes networking requirements so that communication between Pods and nodes “just works.”