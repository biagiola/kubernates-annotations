# Docker vs ContainerD and CLI tools

This lecture clears up why older Kubernetes material talks a lot about **Docker**, while newer material talks about **containerd**, and it also explains the different CLI tools you’ll see (like **ctr**, **nerdctl**, and **crictl**) and when to use each.

At the start of the “container era,” Docker became the dominant tool because it made containers easy to use. When Kubernetes first appeared, it was built to **orchestrate Docker specifically**, so Docker and Kubernetes were tightly coupled and Kubernetes originally only worked with Docker. As Kubernetes became popular, other container runtimes (like **rkt/rocket**) also wanted to work with it, and users wanted Kubernetes to support runtimes beyond Docker. To enable that, Kubernetes introduced the **Container Runtime Interface (CRI)**, which is a standard interface Kubernetes can use to talk to different runtimes.

For CRI to work broadly, the ecosystem standardized around **OCI (Open Container Initiative)** standards. OCI defines two key specs: an **image spec** (how images should be built/packaged) and a **runtime spec** (how a container runtime should behave). The idea is: if a runtime follows OCI standards and exposes CRI support, it can plug into Kubernetes. Runtimes like rkt (and others) could be supported through CRI. Docker, however, wasn’t originally built for CRI because Docker predated CRI and was already widely used. So Kubernetes kept Docker support via a temporary “bridge” called **Dockershim**—basically a workaround that allowed Kubernetes to keep working with Docker even though Docker wasn’t CRI-native.

The key architectural point the instructor makes is that **Docker is more than just a runtime**. Docker bundles multiple pieces: the Docker CLI, Docker API, image build tooling, features around volumes/security, and underneath it uses the OCI runtime (**runc**) plus a daemon that manages container lifecycle—**containerd**. Over time, **containerd became a separate project** that Kubernetes could use directly through CRI, like other runtimes. Maintaining Dockershim added complexity and ongoing maintenance effort, so Kubernetes decided to remove Dockershim; as of **Kubernetes v1.24**, Dockershim was removed and **Docker Engine stopped being a supported Kubernetes runtime**. Importantly, this did *not* break “Docker images”: images built with Docker still run, because Docker images follow OCI image standards, so they continue to work with containerd. The change is about the runtime integration, not the image format.

Then the lecture zooms in on **containerd** itself. Although containerd was historically part of Docker, it is now its **own standalone project**, is part of the **CNCF** (with “graduated” status, as the instructor notes), and can be installed without installing full Docker. The practical implication: if you don’t need Docker’s extra features (like the full Docker developer experience and build workflow on that machine), you can run Kubernetes nodes with **containerd alone**.

That raises the obvious question: if you don’t have Docker installed, how do you run or inspect containers? With containerd you’ll see two main CLIs mentioned:

- **`ctr`** comes with containerd. It can do basics like pulling images and running containers (e.g., `ctr images pull ...`, `ctr run ...`). However, it’s **primarily intended for debugging containerd**, it’s not as user-friendly, and it supports a more limited, lower-level workflow. The instructor’s guidance is: treat `ctr` mostly as a debugging tool, not as the main “day-to-day” way to run containers in production environments.
- **`nerdctl`** is the recommended “Docker-like” CLI for containerd. It aims to feel like Docker: in many cases you can take a Docker command and replace `docker` with `nerdctl` (including common options like `p` for port mappings). The instructor emphasizes that `nerdctl` also exposes newer containerd capabilities sooner than Docker typically does—examples mentioned include working with **encrypted images**, **lazy pulling**, **P2P image distribution**, **image signing and verification**, and **namespaces** (useful in Kubernetes contexts), which aren’t available through Docker in the same way.

After that, the lecture introduces a third tool from the Kubernetes side:

- **`crictl`** is a CLI maintained by the **Kubernetes community** to interact with **CRI-compatible runtimes** (containerd, CRI-O, etc.). This tool is “from the Kubernetes perspective,” meaning it’s meant to talk to whatever runtime Kubernetes is using via CRI, not just containerd specifically. Like `ctr`, `crictl` is mainly positioned as a **debugging/inspection** tool (view containers, logs, exec into containers, list images), not the easiest tool for creating and managing containers as a normal workflow. You *can* create containers with it, but it’s not convenient—and more importantly, it can fight with Kubernetes’ control loop: because **kubelet** is responsible for ensuring the correct set of Pods exist, anything created “out of band” (outside kubelet’s awareness) can get cleaned up. So `crictl` is best thought of as a troubleshooting tool for runtime-level inspection on Kubernetes nodes.

The lecture shows that many `crictl` commands look very similar to Docker’s troubleshooting commands: listing containers is like `docker ps` (you’ll use `crictl ps`), running commands inside containers is like `docker exec` (you’ll use `crictl exec` with similar flags like `-i/-t`), and logs are like `docker logs` (`crictl logs`). One notable difference is that `crictl` is **Pod-aware**, so it can list Pods too (something Docker wasn’t designed around). The instructor notes that, historically, people often SSH’d into worker nodes and used Docker commands to troubleshoot. In containerd-based clusters, you’ll do similar troubleshooting but using **`crictl`** instead.

Finally, the lecture explains runtime endpoint selection for `crictl`. Because `crictl` can target different runtimes, you may need to specify which CRI socket to talk to. If you don’t specify anything, it will try connecting to a sequence of default endpoints (the instructor mentions an order like: Dockershim first, then containerd, then CRI-O, and then a “cri-dockerd” style endpoint). If you want to force a specific runtime, you can set it using a `--runtime-endpoint` option or via an environment variable like `CONTAINER_RUNTIME_ENDPOINT`.

To wrap up, the instructor’s practical summary is: **`ctr`** (containerd community) is mainly for debugging containerd and is limited; **`nerdctl`** (containerd community) is the general-purpose “Docker-like” CLI you’d use to run/manage containers with containerd; and **`crictl`** (Kubernetes community) is a CRI-focused tool mainly for debugging and troubleshooting across any CRI-compatible runtime. Since the course labs used to rely on Docker being installed on nodes for troubleshooting, but now use **containerd**, you should remember to use **`crictl`** for node-level troubleshooting going forward.
