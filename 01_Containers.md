# Containers

In this lecture, the instructor gives a high-level intro to Kubernetes (K8s): a container orchestration platform originally built by Google from its experience running containers in production, and now an open-source project that’s widely considered one of the most popular orchestration technologies. The goal is to understand Kubernetes at a high level, but to do that you first need to understand two core ideas: **containers** and **orchestration**. This specific lecture focuses on containers, using **Docker** as the most common container technology; if you already know Docker, the instructor says you can skip ahead.

To explain *why* Docker matters, the instructor shares a real project scenario: they needed to set up an end-to-end application stack with multiple components—examples include a Node.js web server, a MongoDB database, a messaging system like Redis, and even tools like Ansible. Building and maintaining that stack was painful mainly because of **compatibility**. They had to ensure every service worked with the chosen operating system, and sometimes a particular version of a service simply didn’t work on that OS version, forcing them to reconsider the OS itself. Then came dependency conflicts: one service might require one version of a library, while another service needed a different version. As the system evolved—upgrading components, switching databases, changing versions—this “re-check everything” cycle repeated constantly. This whole messy compatibility web is commonly called the **“matrix from hell.”**

Another major issue was onboarding new developers. Each new person had to follow long setup instructions and run many commands, while also matching the “correct” OS and component versions. On top of that, the team had separate development, test, and production environments, and different developers preferred different operating systems—so it was hard to guarantee the app behaved the same everywhere. Overall, this made building, shipping, and maintaining the application stack unnecessarily difficult. The instructor’s need was clear: a way to isolate components so changes to one wouldn’t break others, and a way to reduce dependence on a specific underlying OS setup.

Docker solved that by allowing each component to run in its own **container**, with its own dependencies and libraries, while still running on the same host (for example, the same VM and operating system). The team could create the Docker configuration once, and then developers could start the stack with something as simple as a `docker run` command. The key idea is that developers no longer need to match a specific OS and dependency setup—**they just need Docker installed**.

The lecture defines containers as **isolated environments**: they can have their own processes/services, networking, and filesystem mounts—similar to virtual machines—but with a crucial difference: containers **share the host OS kernel**. The instructor also notes that containers didn’t start with Docker; container tech has existed for many years with examples like LXC/LXD and others. Docker leverages container primitives (commonly associated with LXC historically) but makes them usable by providing a higher-level toolset that simplifies building and running containers.

To clarify what “sharing the kernel” means, the instructor revisits OS basics: Linux distributions like Ubuntu, Fedora, SUSE, and CentOS are made of (1) the **Linux kernel** and (2) the **user-space software** on top (drivers, UI, compilers, file managers, developer tools, etc.). What differentiates distributions is mostly that software layer, while the underlying Linux kernel concept is common. Because Docker containers share the host’s kernel, a Linux host (say Ubuntu) can run containers based on other Linux distributions (Debian/Fedora/SUSE/CentOS), because they all rely on the Linux kernel. Each container includes the “extra software” that makes that distribution feel like itself, while still using the host kernel.

However, you **cannot** run a Windows-based container directly on a Linux Docker host, because Windows does not share the Linux kernel. You would need Docker running on Windows Server for Windows containers. The instructor anticipates a common confusion: “But I can install Docker on Windows and run Linux containers.” The explanation is that Windows typically runs those Linux containers using a **Linux virtual machine under the hood**—so it’s effectively Linux containers running on Linux, inside a VM, on Windows.

The instructor argues this kernel limitation is not really a disadvantage, because Docker’s purpose is not to virtualize multiple operating systems like a hypervisor does. Docker’s purpose is to **package and ship applications** in a consistent, repeatable way so they can run “anywhere” Docker is available, and be started as many times as needed.

Then the lecture contrasts **virtual machines vs containers**. With containers, you have hardware → host OS → Docker → containers, where each container includes the app plus only its required libraries/dependencies (not a full OS). With VMs, you have hardware → hypervisor → multiple VMs, where each VM includes a full guest OS plus dependencies and the application. Because each VM includes a full OS and kernel, VMs tend to be heavier: higher resource overhead, larger disk footprints (often gigabytes), and slower boot times (minutes). Containers are lighter (often megabytes) and start faster (seconds). On the other hand, VMs provide **stronger isolation** because they don’t share a kernel, while containers share more resources (notably the kernel), which means relatively less isolation.

Importantly, the instructor emphasizes it’s not “containers *or* virtual machines”—it’s often **containers *and* virtual machines**. In large environments, it’s common to run containers on Docker hosts that themselves are virtual machines. This way you combine the operational benefits of virtualization (easy provisioning/decommissioning of hosts) with the agility of containers (fast scaling and deployment). The difference is that you no longer need one VM per application like before; instead, you might run hundreds or thousands of containers per VM/host.

Finally, the lecture explains how organizations typically run containerized apps in practice. Many common applications are already packaged as container images and shared via public registries like **Docker Hub** (and historically “Docker Store”). You can pull images for operating systems, databases, and many tools. Once Docker is installed, starting something can be as simple as running `docker run <image>`, for example running Ansible, MongoDB, Redis, or Node.js from their images. If you need multiple instances of a service, you run multiple containers and put a load balancer in front; if an instance fails, you replace it by destroying that container and launching a new one (the instructor notes there are more advanced solutions for this that the course will cover later).

The lecture also clarifies **image vs container**: an **image** is like a template (similar to a VM template) used to create containers, while a **container** is a running instance of that image with its own isolated environment and processes. If you can’t find an existing image for what you need, you can build your own image and publish it to a registry so others (or your team) can reuse it.

To close, the instructor connects Docker to DevOps workflow: traditionally, developers hand off an application plus a long set of setup instructions to ops, and ops struggles when problems occur because they didn’t build it. With Docker, developers and ops collaborate to turn that setup guide into a **Dockerfile** that captures both sides’ requirements. That Dockerfile builds an image, and that image can run consistently on any host with Docker—so the ops team deploys using the same artifact the developer tested, making behavior much more predictable across environments. The lecture ends by pointing learners to other Docker-focused courses for deeper practice, and then moves on to the next lecture.